{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Board(object):\n",
    "    def __init__(self,width,height,terminal_states,reward_states,teleport_starts,teleport_ends):\n",
    "        self.valid_states = [x+1 for x in range(width*height)]\n",
    "        self.edges = {\n",
    "            \"e\": [(x*width)+1 for x in range(height)],\n",
    "            \"n\": [x+1 for x in range(width)],\n",
    "            \"w\": [(x+1)*width for x in range(height)],\n",
    "            \"s\": [(x+((height-1)*width)+1) for x in range(width)],\n",
    "        }\n",
    "        self.edges['ne'] = self.edges['e']+self.edges['n']\n",
    "        self.edges['nw'] = self.edges['w']+self.edges['n']\n",
    "        self.edges['se'] = self.edges['e']+self.edges['s']\n",
    "        self.edges['sw'] = self.edges['w']+self.edges['s']\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.terminal_states = terminal_states\n",
    "        self.reward_states = reward_states\n",
    "        self.teleport_starts = teleport_starts\n",
    "        self.teleport_ends = teleport_ends\n",
    "        \n",
    "    def draw(self,agent_state):\n",
    "        board_vals = [['_' for x in range(self.width)] for y in range(self.height)]\n",
    "        for state in self.terminal_states:\n",
    "            adj_state = state - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = 'T'\n",
    "        a = list(string.ascii_uppercase)\n",
    "        i = 0\n",
    "        for state in self.reward_states:\n",
    "            adj_state = state - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = a[i]\n",
    "            i += 1\n",
    "        for j,state in enumerate(self.teleport_starts):\n",
    "            adj_state = state - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = a[i]\n",
    "            adj_state = self.teleport_ends[j] - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = a[i]+\"'\"\n",
    "            i += 1\n",
    "        \n",
    "        adj_state = (agent_state-1)\n",
    "        board_vals[int(adj_state/self.width)][adj_state%self.width] = \"*\"\n",
    "        \n",
    "        print('\\n'.join([''.join(['{:3}'.format(item) for item in row]) for row in board_vals]))\n",
    "        \n",
    "    def draw_policy(self,policy_vals):\n",
    "        board_vals = [['_' for x in range(self.width)] for y in range(self.height)]\n",
    "        val_chars = {\n",
    "            'e': '<',\n",
    "            'ne': '\\\\',\n",
    "            'n': '^',\n",
    "            'nw': \"/\",\n",
    "            'w': '>',\n",
    "            'sw': \"\\\\.\",\n",
    "            's': 'v',\n",
    "            'se': \"./\",\n",
    "        }\n",
    "        for i,p in enumerate(policy_vals):\n",
    "            if p in val_chars:\n",
    "                board_vals[int(i/self.width)][i%self.width] = val_chars[p]\n",
    "        \n",
    "        print('\\n'.join([''.join(['{:3}'.format(item) for item in row]) for row in board_vals]))\n",
    "    \n",
    "\n",
    "class GridEnvironment(object):\n",
    "    def __init__(self,width,height,movement_reward,edge_reward,reward_locations,terminal_locations,terminal_reward,starting_points,allow_diag = False):\n",
    "        self.movement_reward = float(movement_reward)\n",
    "        self.edge_reward = float(edge_reward)\n",
    "        self.valid_states = [x+1 for x in range(width*height)]\n",
    "        self.valid_actions = ['e','n','w','s']\n",
    "        if allow_diag:\n",
    "            self.valid_actions = ['e','ne','n','nw','w','sw','s','se']\n",
    "        # reward_locations format: \n",
    "#         [\n",
    "#             start_state: [reward,(end_state)?],  # The end_state is optional. Without it, the agent will move in whatever direction they chose\n",
    "#             ...\n",
    "#         ]\n",
    "        self.reward_locations = reward_locations\n",
    "        self.teleporting_starts = [k for k,v in reward_locations.iteritems() if len(v) == 2]\n",
    "        self.teleporting_ends = [v[1] for k,v in reward_locations.iteritems() if len(v) == 2]\n",
    "        self.board = Board(width,height,terminal_locations,reward_locations.keys(),self.teleporting_starts,self.teleporting_ends)\n",
    "        self.terminal_locations = terminal_locations\n",
    "        self.terminal_reward = float(terminal_reward)\n",
    "        self.starting_points = starting_points\n",
    "        \n",
    "    def initialize_agent(self):\n",
    "        if len(self.starting_points) == 0:\n",
    "            return random.randint(1,(self.board.width*self.board.height + 1))\n",
    "        else:\n",
    "            i = random(0,len(self.starting_points))\n",
    "            return self.starting_points[i]\n",
    "    \n",
    "    def is_terminal_state(self,state):\n",
    "        return state in self.terminal_locations\n",
    "    \n",
    "    def move_result(self,old_state,direction):\n",
    "        if direction not in self.valid_actions:\n",
    "            print \"Invalid direction {}\".format(direction)\n",
    "            raise\n",
    "        \n",
    "        if old_state in self.teleporting_starts:\n",
    "            return self.reward_locations[old_state][1]\n",
    "        elif old_state in self.board.edges[direction]:\n",
    "            return old_state\n",
    "        elif direction == \"e\":\n",
    "            return old_state - 1\n",
    "        elif direction == 'ne':\n",
    "            return old_state - 1 - self.board.width\n",
    "        elif direction == \"n\":\n",
    "            return old_state - self.board.width\n",
    "        elif direction == 'nw':\n",
    "            return old_state - self.board.width + 1\n",
    "        elif direction == \"w\":\n",
    "            return old_state + 1\n",
    "        elif direction == 'sw':\n",
    "            return old_state + self.board.width + 1\n",
    "        elif direction == \"s\":\n",
    "            return old_state + self.board.width\n",
    "        elif direction == \"se\":\n",
    "            return old_state + self.board.width - 1\n",
    "            \n",
    "    def transition_probability(self,old_state,new_state,direction):\n",
    "        if old_state in self.terminal_locations:\n",
    "            return 0.0\n",
    "        elif new_state == self.move_result(old_state,direction):\n",
    "            return 1.0\n",
    "    \n",
    "        return 0.0\n",
    "    \n",
    "    def reward(self,old_state,new_state):\n",
    "        if old_state in self.reward_locations.keys():\n",
    "            return self.reward_locations[old_state][0]\n",
    "        elif new_state in self.terminal_locations:\n",
    "            return self.terminal_reward\n",
    "        else:\n",
    "            return self.movement_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EGreedyPolicy(object):\n",
    "    def __init__(self,epsilon):\n",
    "        self.e = epsilon\n",
    "        \n",
    "    def action(self,state,valid_actions,vals,just_val=False):\n",
    "        r = random.random()\n",
    "        if just_val:\n",
    "            r = 1.0\n",
    "        \n",
    "        if r < self.e:\n",
    "            return valid_actions[random.randint(0,len(valid_actions)-1)]\n",
    "        else:\n",
    "            keys = [\"{}_{}\".format(state,a) for a in valid_actions]\n",
    "            vals = [vals[k] for k in keys if k in vals ]\n",
    "            \n",
    "            if len(vals) == 0:\n",
    "                if just_val:\n",
    "                    return \"NA\"\n",
    "                else:\n",
    "                    return valid_actions[random.randint(0,len(valid_actions)-1)]\n",
    "            else:\n",
    "                max_action = keys[max(range(len(vals)),key=(lambda k: vals[k]))].split(\"_\")[1]\n",
    "                return max_action\n",
    "    \n",
    "\n",
    "class RLAgent(object):\n",
    "    def __init__(self,env,discount_factor,alpha,policy):\n",
    "        self.y = discount_factor\n",
    "        self.policy = policy\n",
    "        self.a = alpha\n",
    "        self.env = env\n",
    "        self.valid_actions = self.env.valid_actions\n",
    "        self.valid_states = self.env.valid_states\n",
    "        self.initialize_vals()\n",
    "\n",
    "    def initialize_vals(self):\n",
    "        self.vals = {}\n",
    "        \n",
    "    def initialize_episode(self):\n",
    "        self.curr_state = self.env.initialize_agent()\n",
    "            \n",
    "    def draw_policy(self):\n",
    "        policy_vals = [self.policy.action(state,self.valid_actions,self.vals,True) for state in self.valid_states]\n",
    "        self.env.board.draw_policy(policy_vals)\n",
    "        \n",
    "    def print_vals(self):\n",
    "        for state in self.valid_states:\n",
    "            print \"{}:\".format(state)\n",
    "            for action in self.valid_actions:\n",
    "                key = \"{}_{}\".format(state,action)\n",
    "                print \"\\t{}: {}\".format(action,(self.vals[key] if key in self.vals else \"NA\"))\n",
    "        \n",
    "    def move(self):\n",
    "        return\n",
    "        \n",
    "    def play_episode(self,draw_board_interval=None,max_iter=None):\n",
    "        self.initialize_episode()\n",
    "        \n",
    "        i = 0\n",
    "        while (self.env.is_terminal_state(self.curr_state) == False) and (max_iter == None or i < max_iter):\n",
    "            self.move()\n",
    "            if draw_board_interval != None and i%draw_board_interval == 0:\n",
    "                print \"Step #{}\".format(i+1)\n",
    "                self.env.board.draw(self.curr_state)\n",
    "                print\n",
    "                print\n",
    "            i += 1\n",
    "            \n",
    "        return\n",
    "    \n",
    "class SarsaAgent(RLAgent):\n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        new_action = self.policy.action(new_state,self.valid_actions,self.vals)\n",
    "        new_key = \"{}_{}\".format(new_state,new_action)\n",
    "        new_state_val = (self.vals[new_key] if new_key in self.vals else 0.0)\n",
    "        \n",
    "        new_val = old_val + self.a * (reward + self.y*new_state_val - old_val)\n",
    "        \n",
    "        self.vals[key] = new_val\n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n",
    "    \n",
    "class QLearningAgent(RLAgent):\n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        keys = [\"{}_{}\".format(new_state,a) for a in self.valid_actions]\n",
    "        vals = [self.vals[k] if k in self.vals else 0.0 for k in keys]\n",
    "        max_val = max(vals)\n",
    "        \n",
    "        new_val = old_val + self.a * (reward + self.y*max_val - old_val)\n",
    "        \n",
    "        self.vals[key] = new_val\n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n",
    "\n",
    "class TDLamAgent(RLAgent):\n",
    "    def __init__(self,env,discount_factor,alpha,policy,lam,elig_cutoff = 0.01):\n",
    "        self.y = discount_factor\n",
    "        self.policy = policy\n",
    "        self.a = alpha\n",
    "        self.l = lam\n",
    "        self.elig_cutoff = elig_cutoff\n",
    "        self.env = env\n",
    "        self.valid_actions = self.env.valid_actions\n",
    "        self.valid_states = self.env.valid_states\n",
    "        self.initialize_vals()\n",
    "    \n",
    "    def initialize_vals(self):\n",
    "        self.vals = {}\n",
    "        self.elig_traces = {}\n",
    "    \n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        new_action = self.policy.action(new_state,self.valid_actions,self.vals)\n",
    "        new_key = \"{}_{}\".format(new_state,new_action)\n",
    "        new_state_val = (self.vals[new_key] if new_key in self.vals else 0.0)\n",
    "        delta = reward + self.y*new_state_val - old_val\n",
    "        \n",
    "        if key not in self.elig_traces:\n",
    "            self.elig_traces[key] = 0\n",
    "        \n",
    "        self.elig_traces[key] += 1\n",
    "        \n",
    "        keys_to_remove = []\n",
    "        for k,v in self.elig_traces.iteritems():\n",
    "            if k not in self.vals:\n",
    "                self.vals[k] = 0.0\n",
    "            \n",
    "            self.vals[k] += self.a * delta * v\n",
    "            self.elig_traces[k] = self.y * self.l * v\n",
    "            if self.elig_traces[k] < self.elig_cutoff:\n",
    "                keys_to_remove.append(k)\n",
    "        \n",
    "        for k in keys_to_remove:\n",
    "            del self.elig_traces[k]\n",
    "        \n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n",
    "\n",
    "class TDQAgent(TDLamAgent):\n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        keys = [\"{}_{}\".format(new_state,a) for a in self.valid_actions]\n",
    "        vals = [self.vals[k] if k in self.vals else 0.0 for k in keys]\n",
    "        max_val = max(vals)\n",
    "        \n",
    "        delta = reward + self.y*max_val - old_val\n",
    "        \n",
    "        if key not in self.elig_traces:\n",
    "            self.elig_traces[key] = 0\n",
    "        \n",
    "        self.elig_traces[key] += 1\n",
    "        \n",
    "        keys_to_remove = []\n",
    "        for k,v in self.elig_traces.iteritems():\n",
    "            if k not in self.vals:\n",
    "                self.vals[k] = 0.0\n",
    "            \n",
    "            self.vals[k] += self.a * delta * v\n",
    "            self.elig_traces[k] = self.y * self.l * v\n",
    "            if self.elig_traces[k] < self.elig_cutoff:\n",
    "                keys_to_remove.append(k)\n",
    "        \n",
    "        for k in keys_to_remove:\n",
    "            del self.elig_traces[k]\n",
    "        \n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridEnvironment(5,6,-1,-1,{},[1,30],0,[],False)\n",
    "policy = EGreedyPolicy(0.05)\n",
    "agent = TDQAgent(grid,0.9,0.35,policy,0.01,0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent.initialize_vals()\n",
    "for x in range(1000):\n",
    "    agent.play_episode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_  <  <  <  <  \n",
      "^  <  ^  ^  v  \n",
      "^  ^  ^  >  v  \n",
      "^  <  >  v  v  \n",
      "^  v  v  >  v  \n",
      ">  >  >  >  _  \n"
     ]
    }
   ],
   "source": [
    "agent.draw_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
