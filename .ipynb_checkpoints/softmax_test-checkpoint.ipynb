{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NArmBandit(object):\n",
    "    def __init__(self,n,mean,var):\n",
    "        self.num_arms = n\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.actions = np.random.normal(mean,var,n)\n",
    "        self.optimal_action = np.where(self.actions==np.max(self.actions))[0]\n",
    "        return\n",
    "    \n",
    "    def reward_for(self,action_index):\n",
    "        reward = self.actions[action_index]\n",
    "        noise = np.random.normal(0,1,1)[0]\n",
    "        return reward + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "                \n",
    "    def update_q_vals(self,action,reward):\n",
    "        self.q_vals[action] = (self.q_vals[action] * self.k_vals[action] + reward) / (self.k_vals[action] + 1)\n",
    "        self.k_vals[action] += 1\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q_vals = [0.0 for x in range(self.num_actions)]\n",
    "        self.k_vals = [0 for x in range(self.num_actions)]\n",
    "\n",
    "class EpsilonGreedyAgent(Agent):\n",
    "    def __init__(self,num_actions,e):\n",
    "        self.num_actions = num_actions\n",
    "        self.reset()\n",
    "        self.epsilon = e\n",
    "        return\n",
    "    \n",
    "    def name(self):\n",
    "        return \"E-Greedy Agent {}\".format(self.epsilon)\n",
    "   \n",
    "    def select_action(self,bandit):\n",
    "        r = random.random()\n",
    "        action = None\n",
    "        if(r < self.epsilon):\n",
    "            action = int(random.random() * self.num_actions)\n",
    "        else:\n",
    "            action = self.q_vals.index(max(self.q_vals))\n",
    "        \n",
    "        reward = bandit.reward_for(action)\n",
    "        self.update_q_vals(action,reward)\n",
    "        \n",
    "        return reward, (1 if action == bandit.optimal_action else 0)\n",
    "    \n",
    "class SoftmaxAgent(Agent):\n",
    "    def __init__(self,num_actions,t):\n",
    "        self.num_actions = num_actions\n",
    "        self.temperature = t\n",
    "        self.reset()\n",
    "        return\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Softmax Agent {}\".format(self.temperature)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q_vals = [0.0 for x in range(self.num_actions)]\n",
    "        self.k_vals = [0 for x in range(self.num_actions)]\n",
    "        self.temp_vals = [1.0 for x in range(self.num_actions)]\n",
    "        self.gibbs_sum = self.num_actions\n",
    "        \n",
    "    def update_temp_vals(self,action):\n",
    "        self.gibbs_sum -= self.temp_vals[action]\n",
    "        self.temp_vals[action] = np.exp(self.q_vals[action]/self.temperature)\n",
    "        self.gibbs_sum += self.temp_vals[action]\n",
    "        return\n",
    "    \n",
    "    def select_action(self,bandit):\n",
    "        r = random.random() * self.gibbs_sum\n",
    "        i = self.temp_vals[0]\n",
    "        action = 0\n",
    "        while i < r and action+1 < len(self.temp_vals):\n",
    "            i += self.temp_vals[action+1]\n",
    "            action += 1\n",
    "        \n",
    "        reward = bandit.reward_for(action)\n",
    "        self.update_q_vals(action,reward)\n",
    "        self.update_temp_vals(action)\n",
    "        \n",
    "        return reward, (1 if action == bandit.optimal_action else 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_bandits = 2000\n",
    "num_steps = 1000\n",
    "bandit_arms = 10\n",
    "bandit_mean = 0.0\n",
    "bandit_var = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent E-Greedy Agent 0.0\n",
      "Running agent E-Greedy Agent 0.01\n",
      "Running agent E-Greedy Agent 0.1\n",
      "Running agent Softmax Agent 0.01\n",
      "Running agent Softmax Agent 0.1\n",
      "Running agent Softmax Agent 1\n"
     ]
    }
   ],
   "source": [
    "agents = [\n",
    "    EpsilonGreedyAgent(num_actions=bandit_arms,e=0.0),\n",
    "    EpsilonGreedyAgent(num_actions=bandit_arms,e=0.01),\n",
    "    EpsilonGreedyAgent(num_actions=bandit_arms,e=0.1),\n",
    "    SoftmaxAgent(num_actions=bandit_arms,t=0.01),\n",
    "    SoftmaxAgent(num_actions=bandit_arms,t=0.1),\n",
    "    SoftmaxAgent(num_actions=bandit_arms,t=1)\n",
    "]\n",
    "\n",
    "bandits = [NArmBandit(n=bandit_arms,mean=bandit_mean,var=bandit_var) for i in range(num_bandits)]\n",
    "\n",
    "rewards = [] # array of 8 agents, with 2k trials of 1k steps for each agent 8 x 2k x 1k\n",
    "optimal_action = [] # then we average the 2k trials to end up with an 8 x 1k array of rewards or actions\n",
    "\n",
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212)\n",
    "x_data = range(num_steps)\n",
    "colors = ['r','g','b','c','y','m','k','r']\n",
    "\n",
    "for a, agent in enumerate(agents):\n",
    "    print \"Running agent {}\".format(agent.name())\n",
    "    rewards = []\n",
    "    opts = []\n",
    "    append_r = rewards.append\n",
    "    append_o = opts.append\n",
    "    for bandit in bandits:\n",
    "        agent.reset()\n",
    "        bandit_results = [agent.select_action(bandit) for t in x_data]\n",
    "        append_r([b[0] for b in bandit_results])\n",
    "        append_o([b[1] for b in bandit_results])\n",
    "        \n",
    "    avg_r = [float(sum(col))/len(col) for col in zip(*rewards)]\n",
    "    avg_o = [float(sum(col))/len(col) for col in zip(*opts)]\n",
    "    \n",
    "    ax1.plot(x_data, avg_r, label=agent.name(),color=colors[a])\n",
    "    ax2.plot(x_data, avg_o, label=agent.name(),color=colors[a])\n",
    "    \n",
    "box1 = ax1.get_position()\n",
    "ax1.set_position([box1.x0, box1.y0, box1.width * 0.7, box1.height])\n",
    "\n",
    "box2 = ax2.get_position()\n",
    "ax2.set_position([box2.x0, box2.y0, box2.width * 0.7, box2.height])\n",
    "\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title(\"Average Reward per Agent\")\n",
    "\n",
    "ax2.set_ylabel('% Opt Action')\n",
    "ax2.set_title(\"% Opt Action per Agent\")\n",
    "\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
