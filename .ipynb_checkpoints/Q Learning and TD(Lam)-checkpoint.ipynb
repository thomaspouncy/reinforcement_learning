{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Board(object):\n",
    "    def __init__(self,width,height,terminal_states,reward_states,teleport_starts,teleport_ends):\n",
    "        self.valid_states = [x+1 for x in range(width*height)]\n",
    "        self.edges = {\n",
    "            \"e\": [(x*width)+1 for x in range(height)],\n",
    "            \"n\": [x+1 for x in range(width)],\n",
    "            \"w\": [(x+1)*width for x in range(height)],\n",
    "            \"s\": [(x+((height-1)*width)+1) for x in range(width)],\n",
    "        }\n",
    "        self.edges['ne'] = self.edges['e']+self.edges['n']\n",
    "        self.edges['nw'] = self.edges['w']+self.edges['n']\n",
    "        self.edges['se'] = self.edges['e']+self.edges['s']\n",
    "        self.edges['sw'] = self.edges['w']+self.edges['s']\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.terminal_states = terminal_states\n",
    "        self.reward_states = reward_states\n",
    "        self.teleport_starts = teleport_starts\n",
    "        self.teleport_ends = teleport_ends\n",
    "        \n",
    "    def draw(self,agent_state):\n",
    "        board_vals = [['_' for x in range(self.width)] for y in range(self.height)]\n",
    "        for state in self.terminal_states:\n",
    "            adj_state = state - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = 'T'\n",
    "        a = list(string.ascii_uppercase)\n",
    "        i = 0\n",
    "        for state in self.reward_states:\n",
    "            adj_state = state - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = a[i]\n",
    "            i += 1\n",
    "        for j,state in enumerate(self.teleport_starts):\n",
    "            adj_state = state - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = a[i]\n",
    "            adj_state = self.teleport_ends[j] - 1\n",
    "            board_vals[int(adj_state/self.width)][adj_state%self.width] = a[i]+\"'\"\n",
    "            i += 1\n",
    "        \n",
    "        adj_state = (agent_state-1)\n",
    "        board_vals[int(adj_state/self.width)][adj_state%self.width] = \"*\"\n",
    "        \n",
    "        print('\\n'.join([''.join(['{:3}'.format(item) for item in row]) for row in board_vals]))\n",
    "        \n",
    "    def draw_policy(self,policy_vals):\n",
    "        board_vals = [['_' for x in range(self.width)] for y in range(self.height)]\n",
    "        val_chars = {\n",
    "            'e': '<',\n",
    "            'ne': '\\\\',\n",
    "            'n': '^',\n",
    "            'nw': \"/\",\n",
    "            'w': '>',\n",
    "            'sw': \"\\\\.\",\n",
    "            's': 'v',\n",
    "            'se': \"./\",\n",
    "        }\n",
    "        for i,p in enumerate(policy_vals):\n",
    "            if p in val_chars:\n",
    "                board_vals[int(i/self.width)][i%self.width] = val_chars[p]\n",
    "        \n",
    "        print('\\n'.join([''.join(['{:3}'.format(item) for item in row]) for row in board_vals]))\n",
    "    \n",
    "\n",
    "class GridEnvironment(object):\n",
    "    def __init__(self,width,height,movement_reward,edge_reward,reward_locations,terminal_locations,terminal_reward,starting_points,allow_diag = False):\n",
    "        self.movement_reward = float(movement_reward)\n",
    "        self.edge_reward = float(edge_reward)\n",
    "        self.valid_states = [x+1 for x in range(width*height)]\n",
    "        self.valid_actions = ['e','n','w','s']\n",
    "        if allow_diag:\n",
    "            self.valid_actions = ['e','ne','n','nw','w','sw','s','se']\n",
    "        # reward_locations format: \n",
    "#         [\n",
    "#             start_state: [reward,(end_state)?],  # The end_state is optional. Without it, the agent will move in whatever direction they chose\n",
    "#             ...\n",
    "#         ]\n",
    "        self.reward_locations = reward_locations\n",
    "        self.teleporting_starts = [k for k,v in reward_locations.iteritems() if len(v) == 2]\n",
    "        self.teleporting_ends = [v[1] for k,v in reward_locations.iteritems() if len(v) == 2]\n",
    "        self.board = Board(width,height,terminal_locations,reward_locations.keys(),self.teleporting_starts,self.teleporting_ends)\n",
    "        self.terminal_locations = terminal_locations\n",
    "        self.terminal_reward = float(terminal_reward)\n",
    "        self.starting_points = starting_points\n",
    "        self.vals = {}\n",
    "        self.optimal_policy = {}\n",
    "        \n",
    "    def initialize_agent(self):\n",
    "        if len(self.starting_points) == 0:\n",
    "            return random.randint(1,(self.board.width*self.board.height + 1))\n",
    "        else:\n",
    "            i = random(0,len(self.starting_points))\n",
    "            return self.starting_points[i]\n",
    "    \n",
    "    def is_terminal_state(self,state):\n",
    "        return state in self.terminal_locations\n",
    "    \n",
    "    def move_result(self,old_state,direction):\n",
    "        if direction not in self.valid_actions:\n",
    "            print \"Invalid direction {}\".format(direction)\n",
    "            raise\n",
    "        \n",
    "        if old_state in self.teleporting_starts:\n",
    "            return self.reward_locations[old_state][1]\n",
    "        elif old_state in self.board.edges[direction]:\n",
    "            return old_state\n",
    "        elif direction == \"e\":\n",
    "            return old_state - 1\n",
    "        elif direction == 'ne':\n",
    "            return old_state - 1 - self.board.width\n",
    "        elif direction == \"n\":\n",
    "            return old_state - self.board.width\n",
    "        elif direction == 'nw':\n",
    "            return old_state - self.board.width + 1\n",
    "        elif direction == \"w\":\n",
    "            return old_state + 1\n",
    "        elif direction == 'sw':\n",
    "            return old_state + self.board.width + 1\n",
    "        elif direction == \"s\":\n",
    "            return old_state + self.board.width\n",
    "        elif direction == \"se\":\n",
    "            return old_state + self.board.width - 1\n",
    "            \n",
    "    def transition_probability(self,old_state,new_state,direction):\n",
    "        if old_state in self.terminal_locations:\n",
    "            return 0.0\n",
    "        elif new_state == self.move_result(old_state,direction):\n",
    "            return 1.0\n",
    "    \n",
    "        return 0.0\n",
    "    \n",
    "    def reward(self,old_state,new_state):\n",
    "        if old_state in self.reward_locations.keys():\n",
    "            return self.reward_locations[old_state][0]\n",
    "        elif new_state in self.terminal_locations:\n",
    "            return self.terminal_reward\n",
    "        else:\n",
    "            return self.movement_reward\n",
    "    \n",
    "    def calculate_q_values(self,y):\n",
    "        vals = {}\n",
    "        policy = {}\n",
    "        thresh = 0.0000001\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in self.valid_states:\n",
    "                max_action = ''\n",
    "                max_val = None\n",
    "                for action in self.valid_actions:\n",
    "                    key = \"{}_{}\".format(state,action)\n",
    "                    if self.is_terminal_state(state):\n",
    "                        new_val = self.terminal_reward\n",
    "                    else:\n",
    "                        new_state = self.move_result(state,action)\n",
    "                        ks = [\"{}_{}\".format(new_state,a) for a in self.valid_actions]\n",
    "                        vs = [vals[k] if k in vals else 0.0 for k in ks]\n",
    "                        max_a = ks[max(range(len(ks)),key=(lambda k: vs[k]))].split(\"_\")[1]\n",
    "\n",
    "                        new_key = \"{}_{}\".format(new_state,max_a)\n",
    "                        new_val = self.reward(state,new_state) + y * (vals[new_key] if new_key in vals else 0.0)\n",
    "\n",
    "                    delta = max([delta,abs(new_val - (vals[key] if key in vals else 0.0))])\n",
    "                    vals[key] = new_val\n",
    "                    if max_val == None or new_val > max_val:\n",
    "                        max_val = new_val\n",
    "                        max_action = action\n",
    "                policy[state] = max_action\n",
    "                \n",
    "\n",
    "#             print delta\n",
    "            if delta < thresh:\n",
    "                break;\n",
    "        \n",
    "        self.vals = vals\n",
    "        self.optimal_policy = policy\n",
    "        \n",
    "        policy_vals = [policy[state] for state in self.valid_states]\n",
    "        self.board.draw_policy(policy_vals)\n",
    "        print \n",
    "        \n",
    "        for state in self.valid_states:\n",
    "            print \"{}:\".format(state)\n",
    "            for action in self.valid_actions:\n",
    "                key = \"{}_{}\".format(state,action)\n",
    "                print \"\\t{}: {}\".format(action,(vals[key] if key in vals else \"NA\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EGreedyPolicy(object):\n",
    "    def __init__(self,epsilon):\n",
    "        self.e = epsilon\n",
    "        \n",
    "    def action(self,state,valid_actions,vals,just_val=False):\n",
    "        r = random.random()\n",
    "        if just_val:\n",
    "            r = 1.0\n",
    "        \n",
    "        if r < self.e:\n",
    "            return valid_actions[random.randint(0,len(valid_actions)-1)]\n",
    "        else:\n",
    "            keys = [\"{}_{}\".format(state,a) for a in valid_actions]\n",
    "            vals = [vals[k] for k in keys if k in vals ]\n",
    "            \n",
    "            if len(vals) == 0:\n",
    "                if just_val:\n",
    "                    return \"NA\"\n",
    "                else:\n",
    "                    return valid_actions[random.randint(0,len(valid_actions)-1)]\n",
    "            else:\n",
    "                max_action = keys[max(range(len(vals)),key=(lambda k: vals[k]))].split(\"_\")[1]\n",
    "                return max_action\n",
    "    \n",
    "\n",
    "class RLAgent(object):\n",
    "    def __init__(self,env,discount_factor,alpha,policy):\n",
    "        self.y = discount_factor\n",
    "        self.policy = policy\n",
    "        self.a = alpha\n",
    "        self.env = env\n",
    "        self.valid_actions = self.env.valid_actions\n",
    "        self.valid_states = self.env.valid_states\n",
    "        self.initialize_vals()\n",
    "\n",
    "    def initialize_vals(self):\n",
    "        self.vals = {}\n",
    "        \n",
    "    def initialize_episode(self):\n",
    "        self.curr_state = self.env.initialize_agent()\n",
    "            \n",
    "    def draw_policy(self):\n",
    "        policy_vals = [self.policy.action(state,self.valid_actions,self.vals,True) for state in self.valid_states]\n",
    "        self.env.board.draw_policy(policy_vals)\n",
    "        \n",
    "    def print_vals(self):\n",
    "        for state in self.valid_states:\n",
    "            print \"{}:\".format(state)\n",
    "            for action in self.valid_actions:\n",
    "                key = \"{}_{}\".format(state,action)\n",
    "                print \"\\t{}: {}\".format(action,(self.vals[key] if key in self.vals else \"NA\"))\n",
    "        \n",
    "    def move(self):\n",
    "        return\n",
    "        \n",
    "    def play_episode(self,draw_board_interval=None,max_iter=None):\n",
    "        self.initialize_episode()\n",
    "        \n",
    "        i = 0\n",
    "        while (self.env.is_terminal_state(self.curr_state) == False) and (max_iter == None or i < max_iter):\n",
    "            self.move()\n",
    "            if draw_board_interval != None and i%draw_board_interval == 0:\n",
    "                print \"Step #{}\".format(i+1)\n",
    "                self.env.board.draw(self.curr_state)\n",
    "                print\n",
    "                print\n",
    "            i += 1\n",
    "            \n",
    "        return\n",
    "    \n",
    "class SarsaAgent(RLAgent):\n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        new_action = self.policy.action(new_state,self.valid_actions,self.vals)\n",
    "        new_key = \"{}_{}\".format(new_state,new_action)\n",
    "        new_state_val = (self.vals[new_key] if new_key in self.vals else 0.0)\n",
    "        \n",
    "        new_val = old_val + self.a * (reward + self.y*new_state_val - old_val)\n",
    "        \n",
    "        self.vals[key] = new_val\n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n",
    "    \n",
    "class QLearningAgent(RLAgent):\n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        ks = [\"{}_{}\".format(new_state,a) for a in self.valid_actions]\n",
    "        vs = [self.vals[k] if k in self.vals else 0.0 for k in ks]\n",
    "        max_val = max(vs)\n",
    "        \n",
    "        new_val = old_val + self.a * (reward + self.y*max_val - old_val)\n",
    "        \n",
    "        self.vals[key] = new_val\n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n",
    "\n",
    "class TDLamAgent(RLAgent):\n",
    "    def __init__(self,env,discount_factor,alpha,policy,lam,elig_cutoff = 0.01):\n",
    "        self.y = discount_factor\n",
    "        self.policy = policy\n",
    "        self.a = alpha\n",
    "        self.l = lam\n",
    "        self.elig_cutoff = elig_cutoff\n",
    "        self.env = env\n",
    "        self.valid_actions = self.env.valid_actions\n",
    "        self.valid_states = self.env.valid_states\n",
    "        self.initialize_vals()\n",
    "    \n",
    "    def initialize_vals(self):\n",
    "        self.vals = {}\n",
    "        self.elig_traces = {}\n",
    "    \n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        new_action = self.policy.action(new_state,self.valid_actions,self.vals)\n",
    "        new_key = \"{}_{}\".format(new_state,new_action)\n",
    "        new_state_val = (self.vals[new_key] if new_key in self.vals else 0.0)\n",
    "        delta = reward + self.y*new_state_val - old_val\n",
    "        \n",
    "        if key not in self.elig_traces:\n",
    "            self.elig_traces[key] = 0\n",
    "        \n",
    "        self.elig_traces[key] += 1\n",
    "        \n",
    "        keys_to_remove = []\n",
    "        for k,v in self.elig_traces.iteritems():\n",
    "            if k not in self.vals:\n",
    "                self.vals[k] = 0.0\n",
    "            \n",
    "            self.vals[k] += self.a * delta * v\n",
    "            self.elig_traces[k] = self.y * self.l * v\n",
    "            if self.elig_traces[k] < self.elig_cutoff:\n",
    "                keys_to_remove.append(k)\n",
    "        \n",
    "        for k in keys_to_remove:\n",
    "            del self.elig_traces[k]\n",
    "        \n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n",
    "\n",
    "class TDQAgent(TDLamAgent):\n",
    "    def move(self):\n",
    "        action = self.policy.action(self.curr_state,self.valid_actions,self.vals)\n",
    "        \n",
    "        key = \"{}_{}\".format(self.curr_state,action)\n",
    "        if key not in self.vals:\n",
    "            self.vals[key] = 0.0\n",
    "\n",
    "        old_val = self.vals[key]\n",
    "        new_state = self.env.move_result(self.curr_state,action)\n",
    "        reward = self.env.reward(self.curr_state,new_state)\n",
    "        \n",
    "        ks = [\"{}_{}\".format(new_state,a) for a in self.valid_actions]\n",
    "        vs = [self.vals[k] if k in self.vals else 0.0 for k in ks]\n",
    "        max_val = max(vs)\n",
    "        \n",
    "        delta = reward + self.y*max_val - old_val\n",
    "        \n",
    "        if key not in self.elig_traces:\n",
    "            self.elig_traces[key] = 0\n",
    "        \n",
    "        self.elig_traces[key] += 1\n",
    "        \n",
    "        keys_to_remove = []\n",
    "        for k,v in self.elig_traces.iteritems():\n",
    "            if k not in self.vals:\n",
    "                self.vals[k] = 0.0\n",
    "            \n",
    "            self.vals[k] += self.a * delta * v\n",
    "            self.elig_traces[k] = self.y * self.l * v\n",
    "            if self.elig_traces[k] < self.elig_cutoff:\n",
    "                keys_to_remove.append(k)\n",
    "        \n",
    "        for k in keys_to_remove:\n",
    "            del self.elig_traces[k]\n",
    "        \n",
    "        self.curr_state = new_state\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(agent,env):\n",
    "    actual_q_vals = env.vals\n",
    "    approx_q_vals = agent.vals\n",
    "    \n",
    "    se = []\n",
    "    for k,v in actual_q_vals.iteritems():\n",
    "        se.append(math.pow((v-(approx_q_vals[k] if k in approx_q_vals else 0.0)),2))\n",
    "    \n",
    "    mse = np.mean(se)\n",
    "    rmse = math.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<  <  <  <  <  \n",
      "^  <  <  <  v  \n",
      "^  <  <  >  v  \n",
      "^  <  >  >  v  \n",
      "^  >  >  >  v  \n",
      ">  >  >  >  <  \n",
      "\n",
      "1:\n",
      "\te: 0.0\n",
      "\tn: 0.0\n",
      "\tw: 0.0\n",
      "\ts: 0.0\n",
      "2:\n",
      "\te: 0.0\n",
      "\tn: -1.0\n",
      "\tw: -1.9\n",
      "\ts: -1.9\n",
      "3:\n",
      "\te: -1.0\n",
      "\tn: -1.9\n",
      "\tw: -2.71\n",
      "\ts: -2.71\n",
      "4:\n",
      "\te: -1.9\n",
      "\tn: -2.71\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "5:\n",
      "\te: -2.71\n",
      "\tn: -3.439\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "6:\n",
      "\te: -1.0\n",
      "\tn: 0.0\n",
      "\tw: -1.9\n",
      "\ts: -1.9\n",
      "7:\n",
      "\te: -1.0\n",
      "\tn: -1.0\n",
      "\tw: -2.71\n",
      "\ts: -2.71\n",
      "8:\n",
      "\te: -1.9\n",
      "\tn: -1.9\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "9:\n",
      "\te: -2.71\n",
      "\tn: -2.71\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "10:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -3.439\n",
      "\ts: -2.71\n",
      "11:\n",
      "\te: -1.9\n",
      "\tn: -1.0\n",
      "\tw: -2.71\n",
      "\ts: -2.71\n",
      "12:\n",
      "\te: -1.9\n",
      "\tn: -1.9\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "13:\n",
      "\te: -2.71\n",
      "\tn: -2.71\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "14:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -2.71\n",
      "\ts: -2.71\n",
      "15:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -2.71\n",
      "\ts: -1.9\n",
      "16:\n",
      "\te: -2.71\n",
      "\tn: -1.9\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "17:\n",
      "\te: -2.71\n",
      "\tn: -2.71\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "18:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -2.71\n",
      "\ts: -2.71\n",
      "19:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -1.9\n",
      "\ts: -1.9\n",
      "20:\n",
      "\te: -2.71\n",
      "\tn: -2.71\n",
      "\tw: -1.9\n",
      "\ts: -1.0\n",
      "21:\n",
      "\te: -3.439\n",
      "\tn: -2.71\n",
      "\tw: -3.439\n",
      "\ts: -3.439\n",
      "22:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -2.71\n",
      "\ts: -2.71\n",
      "23:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -1.9\n",
      "\ts: -1.9\n",
      "24:\n",
      "\te: -2.71\n",
      "\tn: -2.71\n",
      "\tw: -1.0\n",
      "\ts: -1.0\n",
      "25:\n",
      "\te: -1.9\n",
      "\tn: -1.9\n",
      "\tw: -1.0\n",
      "\ts: 0.0\n",
      "26:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -2.71\n",
      "\ts: -3.439\n",
      "27:\n",
      "\te: -3.439\n",
      "\tn: -3.439\n",
      "\tw: -1.9\n",
      "\ts: -2.71\n",
      "28:\n",
      "\te: -2.71\n",
      "\tn: -2.71\n",
      "\tw: -1.0\n",
      "\ts: -1.9\n",
      "29:\n",
      "\te: -1.9\n",
      "\tn: -1.9\n",
      "\tw: 0.0\n",
      "\ts: -1.0\n",
      "30:\n",
      "\te: 0.0\n",
      "\tn: 0.0\n",
      "\tw: 0.0\n",
      "\ts: 0.0\n"
     ]
    }
   ],
   "source": [
    "grid = GridEnvironment(5,6,-1,-1,{},[1,30],0,[],False)\n",
    "policy = EGreedyPolicy(0.05)\n",
    "grid.calculate_q_values(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agents = {\n",
    "    \"td_lam_0.0\": TDLamAgent(grid,0.9,0.2,policy,0.0),\n",
    "    \"td_lam_0.2\": TDLamAgent(grid,0.9,0.2,policy,0.2),\n",
    "    \"td_lam_0.4\": TDLamAgent(grid,0.9,0.2,policy,0.4),\n",
    "    \"td_lam_0.6\": TDLamAgent(grid,0.9,0.2,policy,0.6),\n",
    "    \"td_lam_0.8\": TDLamAgent(grid,0.9,0.2,policy,0.8),\n",
    "    \"td_lam_0.9\": TDLamAgent(grid,0.9,0.2,policy,0.9),\n",
    "    \"td_lam_0.95\": TDLamAgent(grid,0.9,0.2,policy,0.95),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent.initialize_vals()\n",
    "data = {}\n",
    "for agent_name, agent in agents.iteritems():\n",
    "    agent.initialize_vals()\n",
    "    data[agent_name] = []\n",
    "\n",
    "episode_interval = 500\n",
    "num_episodes = 5000\n",
    "x_data = [x for x in range(num_episodes) if x%episode_interval == 0]\n",
    "\n",
    "for x in range(num_episodes):\n",
    "    for agent_name,agent in agents.iteritems():\n",
    "        agent.play_episode()\n",
    "        if x%episode_interval == 0:\n",
    "            data[agent_name].append(rmse(agent,grid))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_lam_0.95:\n",
      "_  <  <  <  <  \n",
      "^  <  ^  ^  ^  \n",
      "^  ^  ^  v  v  \n",
      "^  ^  v  v  v  \n",
      "^  v  >  v  v  \n",
      "v  >  >  >  _  \n",
      "\n",
      "td_lam_0.2:\n",
      "_  <  <  <  <  \n",
      "^  ^  ^  <  v  \n",
      "^  ^  ^  v  v  \n",
      "^  ^  >  v  v  \n",
      "^  >  >  >  v  \n",
      "^  >  ^  >  _  \n",
      "\n",
      "td_lam_0.0:\n",
      "_  <  <  <  <  \n",
      "^  ^  ^  ^  <  \n",
      "^  <  ^  >  v  \n",
      "^  <  >  v  v  \n",
      "^  >  v  v  v  \n",
      ">  >  >  >  _  \n",
      "\n",
      "td_lam_0.6:\n",
      "_  <  <  <  <  \n",
      "^  <  <  <  v  \n",
      "^  ^  <  v  v  \n",
      "^  <  v  v  v  \n",
      "^  v  v  v  v  \n",
      ">  >  >  >  _  \n",
      "\n",
      "td_lam_0.4:\n",
      "_  <  <  <  <  \n",
      "^  ^  ^  ^  ^  \n",
      "^  <  <  v  v  \n",
      "^  ^  v  v  v  \n",
      "^  v  v  >  v  \n",
      ">  >  >  >  _  \n",
      "\n",
      "td_lam_0.8:\n",
      "_  <  <  <  <  \n",
      "^  <  ^  <  <  \n",
      "^  <  ^  >  v  \n",
      "^  ^  v  <  v  \n",
      ">  v  >  >  v  \n",
      ">  >  >  >  _  \n",
      "\n",
      "td_lam_0.9:\n",
      "_  <  <  <  <  \n",
      "^  ^  ^  <  ^  \n",
      "^  <  ^  v  v  \n",
      "^  ^  v  v  v  \n",
      ">  >  v  >  v  \n",
      "^  >  >  >  _  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for agent_name, agent in agents.iteritems():\n",
    "    print \"{}:\".format(agent_name)\n",
    "    agent.draw_policy()\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(111)\n",
    "colors = ['r','g','b','c','y','m','k','r']\n",
    "\n",
    "a = 0\n",
    "for agent_name, agent_data in data.iteritems():\n",
    "    ax1.plot(x_data, agent_data, label=agent_name,color=colors[a])\n",
    "    a += 1\n",
    "    \n",
    "box1 = ax1.get_position()\n",
    "ax1.set_position([box1.x0, box1.y0, box1.width * 0.7, box1.height])\n",
    "\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_xlabel('Episodes')\n",
    "ax1.set_title(\"RMSE After X Episodes\")\n",
    "\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td_lam_fixed_alpha_data = data\n",
    "# Ways to evaluate agents:\n",
    "#     RMS of estimated values relative to actual values (calculated using DP)\n",
    "#     Avg reward per episode\n",
    "#     Episodes per time steps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sarsa_data\n",
    "q_learning_data\n",
    "td_fixed_lam_no_elig_data\n",
    "td_fixed_alpha_no_elig_data\n",
    "tdq_fixed_lam_no_elig_data\n",
    "tdq_fixed_alpha_no_elig_data\n",
    "tdq_fixed_alpha_data\n",
    "td_lam_fixed_alpha_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"sarsa_0.2\": sarsa_data[\"sarsa_0.2\"],\n",
    "    \"q_learning_0.6\": q_learning_data[\"q_learning_0.6\"],\n",
    "    \"td_lam_no_elig_0.2_0.8\": td_fixed_alpha_no_elig_data[\"td_lam_0.8\"],\n",
    "    \"tdq_no_elig_0.8_0.4\": tdq_fixed_alpha_no_elig_data[\"tdq_0.4\"],\n",
    "    \"td_lam_elig_0.2_0.4\": td_lam_fixed_alpha_data[\"td_lam_0.4\"],\n",
    "    \"tdq_elig_0.8_0.6\": tdq_fixed_alpha_data[\"tdq_0.6\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
